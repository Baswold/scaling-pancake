{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fluid Weights: Perpetual Plasticity for Transformers\n",
    "\n",
    "**A Novel Learning System for Continuous Weight Adaptation**\n",
    "\n",
    "This notebook demonstrates a system where transformer weights adapt continuously during inference, without explicit loss functions or training phases.\n",
    "\n",
    "## Key Innovations\n",
    "\n",
    "1. **Attention-Guided Plasticity (AGP)**: Uses attention patterns to guide weight updates\n",
    "2. **Temporal Surprise Minimization (TSM)**: Reduces prediction \"surprise\" without explicit targets\n",
    "3. **Contextual Homeostasis (CH)**: Maintains stable activation statistics\n",
    "4. **Hybrid Update Rules**: Combines Oja, BCM, and energy-based learning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers accelerate bitsandbytes sentencepiece\n",
    "!pip install -q matplotlib pandas seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the fluid_weights package (or upload it)\n",
    "# For Colab, we'll define it inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Create package directory\n",
    "os.makedirs('fluid_weights', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fluid_weights/__init__.py\n",
    "\"\"\"\n",
    "Fluid Weights: Perpetual Plasticity for Transformer Models\n",
    "\"\"\"\n",
    "\n",
    "from .core import FluidLoRA, FluidTransformer, FluidConfig, PlasticityMode\n",
    "from .update_rules import (\n",
    "    UpdateRule, HebbianUpdate, OjaUpdate, BCMUpdate,\n",
    "    PredictiveCodingUpdate, EnergyBasedUpdate, HybridUpdate\n",
    ")\n",
    "from .stability import (\n",
    "    StabilityMechanism, ElasticWeightConsolidation,\n",
    "    SpectralNormConstraint, GradientClipping, AdaptiveRateControl\n",
    ")\n",
    "\n",
    "__version__ = \"1.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fluid_weights/update_rules.py\n",
    "\"\"\"\n",
    "Update Rules for Fluid Weight Learning\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UpdateContext:\n",
    "    \"\"\"Context for update rules.\"\"\"\n",
    "    x: torch.Tensor\n",
    "    h: torch.Tensor\n",
    "    y: torch.Tensor\n",
    "    A: torch.Tensor\n",
    "    B: torch.Tensor\n",
    "    attention_weights: Optional[torch.Tensor] = None\n",
    "    layer_idx: int = 0\n",
    "    step: int = 0\n",
    "\n",
    "\n",
    "class UpdateRule(ABC):\n",
    "    def __init__(self, learning_rate: float = 1e-5, **kwargs):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.config = kwargs\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_update(self, ctx: UpdateContext) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        pass\n",
    "\n",
    "\n",
    "class HebbianUpdate(UpdateRule):\n",
    "    \"\"\"Classical Hebbian: neurons that fire together, wire together.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 1e-5, normalize: bool = True, **kwargs):\n",
    "        super().__init__(learning_rate, **kwargs)\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def compute_update(self, ctx: UpdateContext) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x_flat = ctx.x.reshape(-1, ctx.x.shape[-1])\n",
    "        h_flat = ctx.h.reshape(-1, ctx.h.shape[-1])\n",
    "        y_flat = ctx.y.reshape(-1, ctx.y.shape[-1])\n",
    "\n",
    "        delta_A = x_flat.T @ h_flat\n",
    "        delta_B = h_flat.T @ y_flat\n",
    "\n",
    "        if self.normalize:\n",
    "            x_norm = torch.norm(x_flat) + 1e-8\n",
    "            h_norm = torch.norm(h_flat) + 1e-8\n",
    "            y_norm = torch.norm(y_flat) + 1e-8\n",
    "            delta_A = delta_A / (x_norm * h_norm)\n",
    "            delta_B = delta_B / (h_norm * y_norm)\n",
    "\n",
    "        return self.learning_rate * delta_A, self.learning_rate * delta_B\n",
    "\n",
    "\n",
    "class OjaUpdate(UpdateRule):\n",
    "    \"\"\"Oja's rule: self-normalizing Hebbian learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 1e-5, stabilization_strength: float = 1.0, **kwargs):\n",
    "        super().__init__(learning_rate, **kwargs)\n",
    "        self.stabilization_strength = stabilization_strength\n",
    "\n",
    "    def compute_update(self, ctx: UpdateContext) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x_flat = ctx.x.reshape(-1, ctx.x.shape[-1])\n",
    "        h_flat = ctx.h.reshape(-1, ctx.h.shape[-1])\n",
    "        y_flat = ctx.y.reshape(-1, ctx.y.shape[-1])\n",
    "\n",
    "        hebbian_A = x_flat.T @ h_flat\n",
    "        hebbian_B = h_flat.T @ y_flat\n",
    "\n",
    "        h_norm_sq = torch.mean(torch.sum(h_flat ** 2, dim=-1))\n",
    "        y_norm_sq = torch.mean(torch.sum(y_flat ** 2, dim=-1))\n",
    "\n",
    "        stabilize_A = h_norm_sq * ctx.A\n",
    "        stabilize_B = y_norm_sq * ctx.B\n",
    "\n",
    "        delta_A = hebbian_A / (x_flat.shape[0] + 1e-8) - self.stabilization_strength * stabilize_A\n",
    "        delta_B = hebbian_B / (h_flat.shape[0] + 1e-8) - self.stabilization_strength * stabilize_B\n",
    "\n",
    "        return self.learning_rate * delta_A, self.learning_rate * delta_B\n",
    "\n",
    "\n",
    "class BCMUpdate(UpdateRule):\n",
    "    \"\"\"BCM rule: sliding threshold plasticity.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 1e-5, threshold_decay: float = 0.99, **kwargs):\n",
    "        super().__init__(learning_rate, **kwargs)\n",
    "        self.threshold_decay = threshold_decay\n",
    "        self.theta_h = None\n",
    "        self.theta_y = None\n",
    "\n",
    "    def compute_update(self, ctx: UpdateContext) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x_flat = ctx.x.reshape(-1, ctx.x.shape[-1])\n",
    "        h_flat = ctx.h.reshape(-1, ctx.h.shape[-1])\n",
    "        y_flat = ctx.y.reshape(-1, ctx.y.shape[-1])\n",
    "\n",
    "        h_sq = torch.mean(h_flat ** 2, dim=0)\n",
    "        y_sq = torch.mean(y_flat ** 2, dim=0)\n",
    "\n",
    "        if self.theta_h is None or self.theta_h.shape != h_sq.shape:\n",
    "            self.theta_h = h_sq.detach().clone()\n",
    "        else:\n",
    "            self.theta_h = self.threshold_decay * self.theta_h + (1 - self.threshold_decay) * h_sq.detach()\n",
    "\n",
    "        if self.theta_y is None or self.theta_y.shape != y_sq.shape:\n",
    "            self.theta_y = y_sq.detach().clone()\n",
    "        else:\n",
    "            self.theta_y = self.threshold_decay * self.theta_y + (1 - self.threshold_decay) * y_sq.detach()\n",
    "\n",
    "        h_modulated = h_flat * (h_flat - self.theta_h.unsqueeze(0))\n",
    "        y_modulated = y_flat * (y_flat - self.theta_y.unsqueeze(0))\n",
    "\n",
    "        delta_A = x_flat.T @ h_modulated / (x_flat.shape[0] + 1e-8)\n",
    "        delta_B = h_flat.T @ y_modulated / (h_flat.shape[0] + 1e-8)\n",
    "\n",
    "        return self.learning_rate * delta_A, self.learning_rate * delta_B\n",
    "\n",
    "\n",
    "class PredictiveCodingUpdate(UpdateRule):\n",
    "    \"\"\"Predictive coding: learn by minimizing prediction errors.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 1e-5, **kwargs):\n",
    "        super().__init__(learning_rate, **kwargs)\n",
    "\n",
    "    def compute_update(self, ctx: UpdateContext) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x_flat = ctx.x.reshape(-1, ctx.x.shape[-1])\n",
    "        h_flat = ctx.h.reshape(-1, ctx.h.shape[-1])\n",
    "        y_flat = ctx.y.reshape(-1, ctx.y.shape[-1])\n",
    "\n",
    "        h_pred = y_flat @ ctx.B.T\n",
    "        h_pred = h_pred / (torch.norm(ctx.B, dim=0, keepdim=True).T + 1e-8)\n",
    "        eps_h = h_flat - h_pred\n",
    "\n",
    "        x_pred = h_flat @ ctx.A.T\n",
    "        x_pred = x_pred / (torch.norm(ctx.A, dim=0, keepdim=True).T + 1e-8)\n",
    "        eps_x = x_flat - x_pred\n",
    "\n",
    "        delta_A = eps_x.T @ h_flat / (x_flat.shape[0] + 1e-8)\n",
    "        delta_B = (eps_h.T @ y_flat / (h_flat.shape[0] + 1e-8)).T\n",
    "\n",
    "        return self.learning_rate * delta_A, self.learning_rate * delta_B\n",
    "\n",
    "\n",
    "class EnergyBasedUpdate(UpdateRule):\n",
    "    \"\"\"Energy-based: minimize an energy function.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 1e-5, running_mean_decay: float = 0.99, **kwargs):\n",
    "        super().__init__(learning_rate, **kwargs)\n",
    "        self.running_mean_decay = running_mean_decay\n",
    "        self.h_running_mean = None\n",
    "        self.y_running_mean = None\n",
    "\n",
    "    def compute_update(self, ctx: UpdateContext) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x_flat = ctx.x.reshape(-1, ctx.x.shape[-1])\n",
    "        h_flat = ctx.h.reshape(-1, ctx.h.shape[-1])\n",
    "        y_flat = ctx.y.reshape(-1, ctx.y.shape[-1])\n",
    "\n",
    "        h_mean = torch.mean(h_flat, dim=0)\n",
    "        y_mean = torch.mean(y_flat, dim=0)\n",
    "\n",
    "        if self.h_running_mean is None or self.h_running_mean.shape != h_mean.shape:\n",
    "            self.h_running_mean = h_mean.detach().clone()\n",
    "            self.y_running_mean = y_mean.detach().clone()\n",
    "        else:\n",
    "            self.h_running_mean = self.running_mean_decay * self.h_running_mean + (1 - self.running_mean_decay) * h_mean.detach()\n",
    "            self.y_running_mean = self.running_mean_decay * self.y_running_mean + (1 - self.running_mean_decay) * y_mean.detach()\n",
    "\n",
    "        h_error = h_flat - self.h_running_mean.unsqueeze(0)\n",
    "        y_error = y_flat - self.y_running_mean.unsqueeze(0)\n",
    "\n",
    "        delta_A = -x_flat.T @ h_error / (x_flat.shape[0] + 1e-8)\n",
    "        delta_B = -h_flat.T @ y_error / (h_flat.shape[0] + 1e-8)\n",
    "\n",
    "        return self.learning_rate * delta_A, self.learning_rate * delta_B\n",
    "\n",
    "\n",
    "class HybridUpdate(UpdateRule):\n",
    "    \"\"\"Combine multiple update rules.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 1e-5, rules: Dict = None, **kwargs):\n",
    "        super().__init__(learning_rate, **kwargs)\n",
    "        if rules is None:\n",
    "            self.rules = {\n",
    "                'oja': (OjaUpdate(learning_rate=1.0), 0.5),\n",
    "                'bcm': (BCMUpdate(learning_rate=1.0), 0.3),\n",
    "                'energy': (EnergyBasedUpdate(learning_rate=1.0), 0.2),\n",
    "            }\n",
    "        else:\n",
    "            self.rules = rules\n",
    "\n",
    "    def compute_update(self, ctx: UpdateContext) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        total_delta_A = torch.zeros_like(ctx.A)\n",
    "        total_delta_B = torch.zeros_like(ctx.B)\n",
    "\n",
    "        for name, (rule, weight) in self.rules.items():\n",
    "            delta_A, delta_B = rule.compute_update(ctx)\n",
    "            total_delta_A += weight * delta_A\n",
    "            total_delta_B += weight * delta_B\n",
    "\n",
    "        return self.learning_rate * total_delta_A, self.learning_rate * total_delta_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fluid_weights/stability.py\n",
    "\"\"\"\n",
    "Stability Mechanisms for Fluid Weight Learning\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Optional, Tuple, List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StabilityMetrics:\n",
    "    weight_norm: float = 0.0\n",
    "    spectral_norm: float = 0.0\n",
    "    drift_from_origin: float = 0.0\n",
    "    oscillation_score: float = 0.0\n",
    "\n",
    "\n",
    "class StabilityMechanism(ABC):\n",
    "    @abstractmethod\n",
    "    def constrain_update(self, delta_A, delta_B, A, B) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        pass\n",
    "\n",
    "    def check_stability(self, A, B) -> StabilityMetrics:\n",
    "        return StabilityMetrics(weight_norm=torch.norm(A).item() + torch.norm(B).item())\n",
    "\n",
    "\n",
    "class ElasticWeightConsolidation(StabilityMechanism):\n",
    "    \"\"\"Protect important weights from changing too much.\"\"\"\n",
    "    \n",
    "    def __init__(self, consolidation_strength: float = 0.1, importance_decay: float = 0.999):\n",
    "        self.consolidation_strength = consolidation_strength\n",
    "        self.importance_decay = importance_decay\n",
    "        self.A_original = None\n",
    "        self.B_original = None\n",
    "        self.F_A = None\n",
    "        self.F_B = None\n",
    "\n",
    "    def initialize(self, A, B):\n",
    "        self.A_original = A.detach().clone()\n",
    "        self.B_original = B.detach().clone()\n",
    "        self.F_A = torch.zeros_like(A)\n",
    "        self.F_B = torch.zeros_like(B)\n",
    "\n",
    "    def update_fisher(self, A, B, h):\n",
    "        if self.F_A is None:\n",
    "            self.initialize(A, B)\n",
    "        h_importance = torch.mean(h.reshape(-1, h.shape[-1]) ** 2, dim=0)\n",
    "        new_F_A = torch.outer(torch.ones(A.shape[0], device=A.device), h_importance)\n",
    "        new_F_B = torch.outer(h_importance, torch.ones(B.shape[1], device=B.device))\n",
    "        self.F_A = self.importance_decay * self.F_A + (1 - self.importance_decay) * new_F_A\n",
    "        self.F_B = self.importance_decay * self.F_B + (1 - self.importance_decay) * new_F_B\n",
    "\n",
    "    def constrain_update(self, delta_A, delta_B, A, B):\n",
    "        if self.A_original is None:\n",
    "            self.initialize(A, B)\n",
    "        ewc_A = self.consolidation_strength * self.F_A * (A - self.A_original)\n",
    "        ewc_B = self.consolidation_strength * self.F_B * (B - self.B_original)\n",
    "        return delta_A - ewc_A, delta_B - ewc_B\n",
    "\n",
    "\n",
    "class SpectralNormConstraint(StabilityMechanism):\n",
    "    \"\"\"Keep spectral norm bounded.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_spectral_norm: float = 2.0):\n",
    "        self.max_spectral_norm = max_spectral_norm\n",
    "\n",
    "    def _spectral_norm(self, W):\n",
    "        u = torch.randn(W.shape[1], device=W.device)\n",
    "        u = u / torch.norm(u)\n",
    "        for _ in range(3):\n",
    "            v = W @ u\n",
    "            v = v / (torch.norm(v) + 1e-8)\n",
    "            u = W.T @ v\n",
    "            u = u / (torch.norm(u) + 1e-8)\n",
    "        return torch.norm(W @ u).item()\n",
    "\n",
    "    def constrain_update(self, delta_A, delta_B, A, B):\n",
    "        A_new = A + delta_A\n",
    "        B_new = B + delta_B\n",
    "        sigma_A = self._spectral_norm(A_new)\n",
    "        sigma_B = self._spectral_norm(B_new)\n",
    "        if sigma_A > self.max_spectral_norm:\n",
    "            delta_A = delta_A * self.max_spectral_norm / sigma_A\n",
    "        if sigma_B > self.max_spectral_norm:\n",
    "            delta_B = delta_B * self.max_spectral_norm / sigma_B\n",
    "        return delta_A, delta_B\n",
    "\n",
    "\n",
    "class GradientClipping(StabilityMechanism):\n",
    "    \"\"\"Limit update magnitudes.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_norm: float = 0.1):\n",
    "        self.max_norm = max_norm\n",
    "\n",
    "    def constrain_update(self, delta_A, delta_B, A, B):\n",
    "        total_norm = (torch.norm(delta_A) ** 2 + torch.norm(delta_B) ** 2) ** 0.5\n",
    "        if total_norm > self.max_norm:\n",
    "            scale = self.max_norm / (total_norm + 1e-8)\n",
    "            delta_A = delta_A * scale\n",
    "            delta_B = delta_B * scale\n",
    "        return delta_A, delta_B\n",
    "\n",
    "\n",
    "class AdaptiveRateControl(StabilityMechanism):\n",
    "    \"\"\"Dynamically adjust learning rate based on stability.\"\"\"\n",
    "    \n",
    "    def __init__(self, target_stability: float = 0.1, decay_factor: float = 0.9):\n",
    "        self.target_stability = target_stability\n",
    "        self.decay_factor = decay_factor\n",
    "        self.effective_rate = 1.0\n",
    "        self.change_history = []\n",
    "\n",
    "    def constrain_update(self, delta_A, delta_B, A, B):\n",
    "        change_mag = torch.norm(delta_A).item() + torch.norm(delta_B).item()\n",
    "        self.change_history.append(change_mag)\n",
    "        if len(self.change_history) > 100:\n",
    "            self.change_history.pop(0)\n",
    "        stability_score = sum(self.change_history) / len(self.change_history)\n",
    "        if stability_score > self.target_stability:\n",
    "            self.effective_rate *= self.decay_factor\n",
    "        else:\n",
    "            self.effective_rate = min(1.0, self.effective_rate * 1.01)\n",
    "        return delta_A * self.effective_rate, delta_B * self.effective_rate\n",
    "\n",
    "\n",
    "class CompositeStability(StabilityMechanism):\n",
    "    \"\"\"Combine multiple stability mechanisms.\"\"\"\n",
    "    \n",
    "    def __init__(self, mechanisms: List[StabilityMechanism]):\n",
    "        self.mechanisms = mechanisms\n",
    "\n",
    "    def constrain_update(self, delta_A, delta_B, A, B):\n",
    "        for mech in self.mechanisms:\n",
    "            delta_A, delta_B = mech.constrain_update(delta_A, delta_B, A, B)\n",
    "        return delta_A, delta_B\n",
    "\n",
    "    def check_stability(self, A, B):\n",
    "        metrics = StabilityMetrics()\n",
    "        for mech in self.mechanisms:\n",
    "            m = mech.check_stability(A, B)\n",
    "            metrics.weight_norm = max(metrics.weight_norm, m.weight_norm)\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile fluid_weights/core.py\n",
    "\"\"\"\n",
    "Core Fluid Weight Learning System\n",
    "\n",
    "NOVEL CONTRIBUTIONS:\n",
    "1. Attention-Guided Plasticity (AGP)\n",
    "2. Temporal Surprise Minimization (TSM)\n",
    "3. Contextual Homeostasis (CH)\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, Any, Optional, Tuple, List\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "import math\n",
    "import re\n",
    "\n",
    "from .update_rules import UpdateRule, UpdateContext, HybridUpdate, OjaUpdate, BCMUpdate, EnergyBasedUpdate, PredictiveCodingUpdate\n",
    "from .stability import StabilityMechanism, CompositeStability, ElasticWeightConsolidation, SpectralNormConstraint, GradientClipping, AdaptiveRateControl\n",
    "\n",
    "\n",
    "class PlasticityMode(Enum):\n",
    "    FROZEN = \"frozen\"\n",
    "    FLUID = \"fluid\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FluidConfig:\n",
    "    rank: int = 16\n",
    "    alpha: float = 32.0\n",
    "    dropout: float = 0.0\n",
    "    mode: PlasticityMode = PlasticityMode.FLUID\n",
    "    learning_rate: float = 1e-5\n",
    "    update_every_n_tokens: int = 1\n",
    "    use_ewc: bool = True\n",
    "    ewc_strength: float = 0.05\n",
    "    use_spectral_norm: bool = True\n",
    "    max_spectral_norm: float = 2.0\n",
    "    use_gradient_clipping: bool = True\n",
    "    max_gradient_norm: float = 0.1\n",
    "    use_adaptive_rate: bool = True\n",
    "    weight_decay_to_origin: float = 0.001\n",
    "    use_attention_guided_plasticity: bool = True\n",
    "    attention_plasticity_strength: float = 0.3\n",
    "    use_temporal_surprise: bool = True\n",
    "    surprise_window: int = 32\n",
    "    use_contextual_homeostasis: bool = True\n",
    "    homeostasis_strength: float = 0.1\n",
    "    homeostasis_burnin: int = 100\n",
    "    track_metrics: bool = True\n",
    "    log_every_n_steps: int = 100\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FluidState:\n",
    "    step: int = 0\n",
    "    total_tokens: int = 0\n",
    "    is_burned_in: bool = False\n",
    "    activation_mean: Optional[torch.Tensor] = None\n",
    "    activation_var: Optional[torch.Tensor] = None\n",
    "    target_mean: Optional[torch.Tensor] = None\n",
    "    target_var: Optional[torch.Tensor] = None\n",
    "    temporal_buffer: Optional[torch.Tensor] = None\n",
    "    metrics_history: List[Dict] = field(default_factory=list)\n",
    "\n",
    "\n",
    "class FluidLoRA(nn.Module):\n",
    "    \"\"\"Fluid LoRA with perpetual plasticity.\"\"\"\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, config: FluidConfig, layer_type: str = \"linear\", layer_idx: int = 0):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.config = config\n",
    "        self.layer_type = layer_type\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "        self.A = nn.Parameter(torch.zeros(in_features, config.rank))\n",
    "        self.B = nn.Parameter(torch.zeros(config.rank, out_features))\n",
    "        self.scaling = config.alpha / config.rank\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.B)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.dropout) if config.dropout > 0 else nn.Identity()\n",
    "        self._init_update_rules()\n",
    "        self._init_stability()\n",
    "        self.state = FluidState()\n",
    "\n",
    "    def _init_update_rules(self):\n",
    "        rules = {\n",
    "            'oja': (OjaUpdate(learning_rate=1.0), 0.4),\n",
    "            'bcm': (BCMUpdate(learning_rate=1.0), 0.2),\n",
    "            'energy': (EnergyBasedUpdate(learning_rate=1.0), 0.2),\n",
    "            'predictive': (PredictiveCodingUpdate(learning_rate=1.0), 0.2),\n",
    "        }\n",
    "        self.base_update = HybridUpdate(learning_rate=self.config.learning_rate, rules=rules)\n",
    "\n",
    "    def _init_stability(self):\n",
    "        mechanisms = []\n",
    "        if self.config.use_ewc:\n",
    "            mechanisms.append(ElasticWeightConsolidation(consolidation_strength=self.config.ewc_strength))\n",
    "        if self.config.use_spectral_norm:\n",
    "            mechanisms.append(SpectralNormConstraint(max_spectral_norm=self.config.max_spectral_norm))\n",
    "        if self.config.use_gradient_clipping:\n",
    "            mechanisms.append(GradientClipping(max_norm=self.config.max_gradient_norm))\n",
    "        if self.config.use_adaptive_rate:\n",
    "            mechanisms.append(AdaptiveRateControl())\n",
    "        self.stability = CompositeStability(mechanisms)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attention_weights: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        h = x @ self.A\n",
    "        h = self.dropout(h)\n",
    "        y = h @ self.B\n",
    "        y = y * self.scaling\n",
    "\n",
    "        if self.config.mode == PlasticityMode.FLUID:\n",
    "            self._apply_fluid_update(x.detach(), h.detach(), y.detach(), attention_weights)\n",
    "\n",
    "        self.state.step += 1\n",
    "        self.state.total_tokens += x.shape[0] * x.shape[1]\n",
    "        return y\n",
    "\n",
    "    def _apply_fluid_update(self, x, h, y, attention_weights=None):\n",
    "        if self.state.step % self.config.update_every_n_tokens != 0:\n",
    "            return\n",
    "\n",
    "        ctx = UpdateContext(x=x, h=h, y=y, A=self.A.data, B=self.B.data, attention_weights=attention_weights, layer_idx=self.layer_idx, step=self.state.step)\n",
    "        delta_A, delta_B = self.base_update.compute_update(ctx)\n",
    "\n",
    "        if self.config.use_attention_guided_plasticity and attention_weights is not None:\n",
    "            agp_A, agp_B = self._attention_guided_update(x, h, y, attention_weights)\n",
    "            delta_A = delta_A + self.config.attention_plasticity_strength * agp_A\n",
    "            delta_B = delta_B + self.config.attention_plasticity_strength * agp_B\n",
    "\n",
    "        if self.config.use_temporal_surprise:\n",
    "            ts_A, ts_B = self._temporal_surprise_update(x, h, y)\n",
    "            delta_A = delta_A + ts_A\n",
    "            delta_B = delta_B + ts_B\n",
    "\n",
    "        if self.config.use_contextual_homeostasis:\n",
    "            ch_A, ch_B = self._homeostasis_update(x, h, y)\n",
    "            delta_A = delta_A + self.config.homeostasis_strength * ch_A\n",
    "            delta_B = delta_B + self.config.homeostasis_strength * ch_B\n",
    "\n",
    "        delta_A, delta_B = self.stability.constrain_update(delta_A, delta_B, self.A.data, self.B.data)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.A.data.add_(delta_A)\n",
    "            self.B.data.add_(delta_B)\n",
    "\n",
    "        if self.config.use_ewc:\n",
    "            for mech in self.stability.mechanisms:\n",
    "                if isinstance(mech, ElasticWeightConsolidation):\n",
    "                    mech.update_fisher(self.A.data, self.B.data, h)\n",
    "\n",
    "        if self.config.track_metrics and self.state.step % self.config.log_every_n_steps == 0:\n",
    "            self.state.metrics_history.append({\n",
    "                'step': self.state.step,\n",
    "                'A_norm': torch.norm(self.A.data).item(),\n",
    "                'B_norm': torch.norm(self.B.data).item(),\n",
    "                'delta_norm': torch.norm(delta_A).item() + torch.norm(delta_B).item(),\n",
    "            })\n",
    "\n",
    "    def _attention_guided_update(self, x, h, y, attn):\n",
    "        attn_avg = attn.mean(dim=(0, 1))\n",
    "        x_flat = x.reshape(-1, x.shape[-1])\n",
    "        h_flat = h.reshape(-1, h.shape[-1])\n",
    "        y_flat = y.reshape(-1, y.shape[-1])\n",
    "        batch_size, seq_len = x.shape[:2]\n",
    "\n",
    "        if self.layer_type in ['query', 'key']:\n",
    "            h_seq = h.reshape(batch_size, seq_len, -1)\n",
    "            h_attended = torch.einsum('ij,bjr->bir', attn_avg, h_seq).reshape(-1, h.shape[-1])\n",
    "            delta_A = x_flat.T @ (h_attended - h_flat) / (x_flat.shape[0] + 1e-8) * self.config.learning_rate\n",
    "            attn_entropy = -torch.sum(attn_avg * torch.log(attn_avg + 1e-8), dim=-1)\n",
    "            confidence = 1.0 / (1.0 + attn_entropy.mean())\n",
    "            delta_B = confidence * h_flat.T @ y_flat / (h_flat.shape[0] + 1e-8) * self.config.learning_rate\n",
    "        elif self.layer_type == 'value':\n",
    "            y_seq = y.reshape(batch_size, seq_len, -1)\n",
    "            y_attended = torch.einsum('ij,bjd->bid', attn_avg, y_seq).reshape(-1, y.shape[-1])\n",
    "            delta_B = h_flat.T @ y_attended / (h_flat.shape[0] + 1e-8) * self.config.learning_rate\n",
    "            delta_A = x_flat.T @ h_flat / (x_flat.shape[0] + 1e-8) * self.config.learning_rate * 0.5\n",
    "        else:\n",
    "            delta_A = x_flat.T @ h_flat / (x_flat.shape[0] + 1e-8) * self.config.learning_rate\n",
    "            delta_B = h_flat.T @ y_flat / (h_flat.shape[0] + 1e-8) * self.config.learning_rate\n",
    "\n",
    "        return delta_A, delta_B\n",
    "\n",
    "    def _temporal_surprise_update(self, x, h, y):\n",
    "        h_flat = h.reshape(-1, h.shape[-1])\n",
    "        x_flat = x.reshape(-1, x.shape[-1])\n",
    "        y_flat = y.reshape(-1, y.shape[-1])\n",
    "\n",
    "        if self.state.temporal_buffer is None:\n",
    "            buffer_size = min(self.config.surprise_window, h_flat.shape[0])\n",
    "            self.state.temporal_buffer = torch.zeros(buffer_size, h_flat.shape[-1], device=h.device)\n",
    "\n",
    "        buffer_size = self.state.temporal_buffer.shape[0]\n",
    "        n_samples = min(h_flat.shape[0], buffer_size)\n",
    "        if n_samples < buffer_size:\n",
    "            self.state.temporal_buffer = torch.roll(self.state.temporal_buffer, -n_samples, dims=0)\n",
    "        self.state.temporal_buffer[-n_samples:] = h_flat[:n_samples].detach()\n",
    "\n",
    "        h_expected = self.state.temporal_buffer.mean(dim=0)\n",
    "        h_std = self.state.temporal_buffer.std(dim=0) + 1e-8\n",
    "        h_current = h_flat.mean(dim=0)\n",
    "        surprise = (h_current - h_expected) / h_std\n",
    "        surprise_weight = torch.sigmoid(torch.norm(surprise) - 1.0)\n",
    "\n",
    "        h_error = surprise.unsqueeze(0)\n",
    "        delta_A = -x_flat.T @ h_error.expand(x_flat.shape[0], -1) / (x_flat.shape[0] + 1e-8) * self.config.learning_rate * surprise_weight\n",
    "\n",
    "        y_expected = self.state.temporal_buffer @ self.B.data\n",
    "        y_error = (y_flat.mean(dim=0) - y_expected.mean(dim=0)).unsqueeze(0)\n",
    "        delta_B = -h_flat.T @ y_error.expand(h_flat.shape[0], -1) / (h_flat.shape[0] + 1e-8) * self.config.learning_rate * surprise_weight\n",
    "\n",
    "        return delta_A, delta_B\n",
    "\n",
    "    def _homeostasis_update(self, x, h, y):\n",
    "        h_flat = h.reshape(-1, h.shape[-1])\n",
    "        x_flat = x.reshape(-1, x.shape[-1])\n",
    "\n",
    "        current_mean = h_flat.mean(dim=0)\n",
    "        current_var = h_flat.var(dim=0)\n",
    "\n",
    "        decay = 0.99\n",
    "        if self.state.activation_mean is None:\n",
    "            self.state.activation_mean = current_mean.detach()\n",
    "            self.state.activation_var = current_var.detach()\n",
    "        else:\n",
    "            self.state.activation_mean = decay * self.state.activation_mean + (1 - decay) * current_mean.detach()\n",
    "            self.state.activation_var = decay * self.state.activation_var + (1 - decay) * current_var.detach()\n",
    "\n",
    "        if self.state.step < self.config.homeostasis_burnin:\n",
    "            if self.state.step == self.config.homeostasis_burnin - 1:\n",
    "                self.state.target_mean = self.state.activation_mean.clone()\n",
    "                self.state.target_var = self.state.activation_var.clone()\n",
    "                self.state.is_burned_in = True\n",
    "            return torch.zeros_like(self.A.data), torch.zeros_like(self.B.data)\n",
    "\n",
    "        mean_error = current_mean - self.state.target_mean\n",
    "        var_ratio = torch.sqrt(self.state.target_var / (current_var + 1e-8))\n",
    "        total_correction = -mean_error.unsqueeze(0) + 0.1 * (var_ratio - 1.0).unsqueeze(0) * h_flat.mean(dim=0, keepdim=True)\n",
    "\n",
    "        delta_A = x_flat.T @ total_correction.expand(x_flat.shape[0], -1) / (x_flat.shape[0] + 1e-8) * self.config.learning_rate\n",
    "        delta_B = (h_flat.T @ (-mean_error.unsqueeze(0)).expand(h_flat.shape[0], -1) @ self.B.data.T).T * self.config.learning_rate * 0.1\n",
    "\n",
    "        return delta_A, delta_B\n",
    "\n",
    "    def save_state(self):\n",
    "        return {'A': self.A.data.clone(), 'B': self.B.data.clone(), 'step': self.state.step}\n",
    "\n",
    "    def load_state(self, saved):\n",
    "        self.A.data = saved['A']\n",
    "        self.B.data = saved['B']\n",
    "        self.state.step = saved['step']\n",
    "\n",
    "\n",
    "class FluidTransformer(nn.Module):\n",
    "    \"\"\"Wrapper for HuggingFace transformers with fluid LoRA.\"\"\"\n",
    "\n",
    "    def __init__(self, model: nn.Module, config: FluidConfig = None, target_modules: List[str] = None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.config = config or FluidConfig()\n",
    "        self.target_modules = target_modules or ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
    "        self.fluid_loras: Dict[str, FluidLoRA] = {}\n",
    "        self._attention_weights: Dict[int, torch.Tensor] = {}\n",
    "        self._patch_model()\n",
    "        self._register_attention_hooks()\n",
    "\n",
    "    def _patch_model(self):\n",
    "        for name, module in self.model.named_modules():\n",
    "            module_name = name.split('.')[-1]\n",
    "            if module_name not in self.target_modules:\n",
    "                continue\n",
    "            if not isinstance(module, nn.Linear):\n",
    "                continue\n",
    "\n",
    "            layer_type = self._infer_layer_type(module_name)\n",
    "            layer_idx = self._infer_layer_idx(name)\n",
    "\n",
    "            fluid_lora = FluidLoRA(\n",
    "                in_features=module.in_features,\n",
    "                out_features=module.out_features,\n",
    "                config=self.config,\n",
    "                layer_type=layer_type,\n",
    "                layer_idx=layer_idx,\n",
    "            ).to(module.weight.device)\n",
    "\n",
    "            lora_name = name.replace('.', '_')\n",
    "            self.fluid_loras[lora_name] = fluid_lora\n",
    "            self._patch_module_forward(name, module, fluid_lora, layer_idx)\n",
    "\n",
    "        print(f\"Patched {len(self.fluid_loras)} modules with FluidLoRA\")\n",
    "\n",
    "    def _infer_layer_type(self, module_name):\n",
    "        if 'q_proj' in module_name or 'query' in module_name:\n",
    "            return 'query'\n",
    "        elif 'k_proj' in module_name or 'key' in module_name:\n",
    "            return 'key'\n",
    "        elif 'v_proj' in module_name or 'value' in module_name:\n",
    "            return 'value'\n",
    "        elif 'o_proj' in module_name or 'out' in module_name:\n",
    "            return 'output'\n",
    "        return 'linear'\n",
    "\n",
    "    def _infer_layer_idx(self, name):\n",
    "        match = re.search(r'\\.(\\d+)\\.', name)\n",
    "        return int(match.group(1)) if match else 0\n",
    "\n",
    "    def _patch_module_forward(self, name, module, fluid_lora, layer_idx):\n",
    "        original_forward = module.forward\n",
    "        def new_forward(x):\n",
    "            out = original_forward(x)\n",
    "            attn = self._attention_weights.get(layer_idx)\n",
    "            lora_out = fluid_lora(x, attention_weights=attn)\n",
    "            return out + lora_out\n",
    "        module.forward = new_forward\n",
    "\n",
    "    def _register_attention_hooks(self):\n",
    "        def make_hook(layer_idx):\n",
    "            def hook(module, input, output):\n",
    "                if isinstance(output, tuple) and len(output) > 1 and output[1] is not None:\n",
    "                    self._attention_weights[layer_idx] = output[1].detach()\n",
    "            return hook\n",
    "        for name, module in self.model.named_modules():\n",
    "            if 'attention' in name.lower() and hasattr(module, 'forward'):\n",
    "                layer_idx = self._infer_layer_idx(name)\n",
    "                module.register_forward_hook(make_hook(layer_idx))\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        self._attention_weights.clear()\n",
    "        kwargs['output_attentions'] = True\n",
    "        return self.model(*args, **kwargs)\n",
    "\n",
    "    def generate(self, *args, **kwargs):\n",
    "        return self.model.generate(*args, **kwargs)\n",
    "\n",
    "    def set_plasticity_mode(self, mode: PlasticityMode):\n",
    "        for lora in self.fluid_loras.values():\n",
    "            lora.config.mode = mode\n",
    "\n",
    "    def freeze(self):\n",
    "        self.set_plasticity_mode(PlasticityMode.FROZEN)\n",
    "\n",
    "    def unfreeze(self):\n",
    "        self.set_plasticity_mode(PlasticityMode.FLUID)\n",
    "\n",
    "    def save_fluid_state(self):\n",
    "        return {name: lora.save_state() for name, lora in self.fluid_loras.items()}\n",
    "\n",
    "    def load_fluid_state(self, saved):\n",
    "        for name, state in saved.items():\n",
    "            if name in self.fluid_loras:\n",
    "                self.fluid_loras[name].load_state(state)\n",
    "\n",
    "    def get_metrics(self):\n",
    "        metrics = []\n",
    "        for name, lora in self.fluid_loras.items():\n",
    "            for m in lora.state.metrics_history:\n",
    "                m['module'] = name\n",
    "                metrics.append(m)\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the module\n",
    "import importlib\n",
    "import fluid_weights\n",
    "importlib.reload(fluid_weights)\n",
    "\n",
    "from fluid_weights import FluidConfig, FluidTransformer, PlasticityMode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Choose model based on available GPU memory\n",
    "# For A100 40GB, we can use Mistral-7B or Llama-2-7B\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"  # or \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded on {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Fluid Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the fluid learning system\n",
    "config = FluidConfig(\n",
    "    # LoRA settings\n",
    "    rank=16,\n",
    "    alpha=32.0,\n",
    "    \n",
    "    # Plasticity settings\n",
    "    learning_rate=1e-5,  # Very small for stability\n",
    "    update_every_n_tokens=1,\n",
    "    \n",
    "    # Stability\n",
    "    use_ewc=True,\n",
    "    ewc_strength=0.05,\n",
    "    use_spectral_norm=True,\n",
    "    max_spectral_norm=2.0,\n",
    "    use_gradient_clipping=True,\n",
    "    max_gradient_norm=0.1,\n",
    "    use_adaptive_rate=True,\n",
    "    \n",
    "    # Novel mechanisms\n",
    "    use_attention_guided_plasticity=True,\n",
    "    attention_plasticity_strength=0.3,\n",
    "    use_temporal_surprise=True,\n",
    "    surprise_window=32,\n",
    "    use_contextual_homeostasis=True,\n",
    "    homeostasis_strength=0.1,\n",
    "    homeostasis_burnin=100,\n",
    "    \n",
    "    # Monitoring\n",
    "    track_metrics=True,\n",
    "    log_every_n_steps=50,\n",
    ")\n",
    "\n",
    "print(\"Config created:\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"  Rank: {config.rank}\")\n",
    "print(f\"  Novel mechanisms: AGP={config.use_attention_guided_plasticity}, TSM={config.use_temporal_surprise}, CH={config.use_contextual_homeostasis}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the model with FluidTransformer\n",
    "fluid_model = FluidTransformer(\n",
    "    model,\n",
    "    config=config,\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    ")\n",
    "\n",
    "print(f\"\\nFluidLoRA modules created: {len(fluid_model.fluid_loras)}\")\n",
    "print(\"\\nFirst few modules:\")\n",
    "for i, (name, lora) in enumerate(list(fluid_model.fluid_loras.items())[:4]):\n",
    "    print(f\"  {name}: {lora.in_features} -> {lora.out_features} (rank={lora.config.rank})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Test (Frozen Weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_new_tokens=100):\n",
    "    \"\"\"Generate a response from the model.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with frozen weights (baseline)\n",
    "fluid_model.freeze()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE TEST (Frozen Weights)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_prompts = [\n",
    "    \"The capital of France is\",\n",
    "    \"Write a haiku about programming:\",\n",
    "    \"Explain quantum computing in simple terms:\",\n",
    "]\n",
    "\n",
    "baseline_responses = []\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    response = generate_response(fluid_model, tokenizer, prompt, max_new_tokens=50)\n",
    "    print(f\"Response: {response[len(prompt):]}\")\n",
    "    baseline_responses.append(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fluid Learning Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable fluid plasticity\n",
    "fluid_model.unfreeze()\n",
    "\n",
    "print(\"Plasticity ENABLED - weights will now adapt during inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data: Expose the model to a specific topic/style\n",
    "# We'll use a series of prompts about a fictional topic to see if the model adapts\n",
    "\n",
    "adaptation_texts = [\n",
    "    \"The kingdom of Zephyria is located in the northern mountains.\",\n",
    "    \"In Zephyria, the people celebrate the Festival of Winds every spring.\",\n",
    "    \"The capital of Zephyria is called Aeropolis.\",\n",
    "    \"Queen Ventara has ruled Zephyria for 40 years.\",\n",
    "    \"Zephyrian currency is called the Gust.\",\n",
    "    \"The national animal of Zephyria is the Silver Eagle.\",\n",
    "    \"Zephyrians are known for their skill in wind magic.\",\n",
    "    \"The Great Library of Aeropolis contains ancient scrolls of air wisdom.\",\n",
    "    \"Traditional Zephyrian food includes cloud bread and sky berries.\",\n",
    "    \"The Zephyrian language has over 100 words for different types of wind.\",\n",
    "]\n",
    "\n",
    "print(f\"Adaptation data: {len(adaptation_texts)} passages about Zephyria\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Process adaptation texts multiple times to allow learning\n",
    "num_epochs = 5\n",
    "\n",
    "print(f\"\\nRunning {num_epochs} epochs of fluid adaptation...\")\n",
    "print(\"(Weights are updating in real-time during forward passes)\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    for text in tqdm(adaptation_texts):\n",
    "        # Tokenize and process\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        # Forward pass triggers fluid updates\n",
    "        with torch.no_grad():\n",
    "            outputs = fluid_model(**inputs)\n",
    "\n",
    "print(\"\\nAdaptation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check metrics\n",
    "metrics = fluid_model.get_metrics()\n",
    "print(f\"\\nCollected {len(metrics)} metric records\")\n",
    "\n",
    "if metrics:\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(metrics)\n",
    "    print(\"\\nMetrics summary:\")\n",
    "    print(df.groupby('module').agg({\n",
    "        'A_norm': ['mean', 'std'],\n",
    "        'B_norm': ['mean', 'std'],\n",
    "        'delta_norm': ['mean', 'max'],\n",
    "    }).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now test if the model has adapted\n",
    "# Freeze weights for fair comparison\n",
    "fluid_model.freeze()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"POST-ADAPTATION TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_prompts_zephyria = [\n",
    "    \"The capital of Zephyria is\",\n",
    "    \"Tell me about the Kingdom of Zephyria:\",\n",
    "    \"What do Zephyrians eat?\",\n",
    "    \"Who rules Zephyria?\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts_zephyria:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    response = generate_response(fluid_model, tokenizer, prompt, max_new_tokens=50)\n",
    "    print(f\"Response: {response[len(prompt):]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that base capabilities are preserved\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BASE CAPABILITY PRESERVATION TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    response = generate_response(fluid_model, tokenizer, prompt, max_new_tokens=50)\n",
    "    print(f\"Response: {response[len(prompt):]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "if metrics:\n",
    "    df = pd.DataFrame(metrics)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Weight norms over time\n",
    "    ax = axes[0, 0]\n",
    "    for module in df['module'].unique()[:5]:  # First 5 modules\n",
    "        module_df = df[df['module'] == module]\n",
    "        ax.plot(module_df['step'], module_df['A_norm'], label=f'{module[:30]}...')\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('A Norm')\n",
    "    ax.set_title('LoRA A Matrix Norms Over Time')\n",
    "    ax.legend(fontsize=8)\n",
    "    \n",
    "    ax = axes[0, 1]\n",
    "    for module in df['module'].unique()[:5]:\n",
    "        module_df = df[df['module'] == module]\n",
    "        ax.plot(module_df['step'], module_df['B_norm'], label=f'{module[:30]}...')\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('B Norm')\n",
    "    ax.set_title('LoRA B Matrix Norms Over Time')\n",
    "    ax.legend(fontsize=8)\n",
    "    \n",
    "    # Update magnitudes\n",
    "    ax = axes[1, 0]\n",
    "    ax.hist(df['delta_norm'], bins=50, alpha=0.7)\n",
    "    ax.set_xlabel('Update Magnitude')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Distribution of Update Magnitudes')\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    # Updates over time\n",
    "    ax = axes[1, 1]\n",
    "    ax.plot(df.groupby('step')['delta_norm'].mean())\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Mean Update Magnitude')\n",
    "    ax.set_title('Average Update Magnitude Over Time')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fluid_weights_metrics.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nMetrics visualization saved to 'fluid_weights_metrics.png'\")\n",
    "else:\n",
    "    print(\"No metrics available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save and Load Adapted State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the adapted state\n",
    "adapted_state = fluid_model.save_fluid_state()\n",
    "\n",
    "print(f\"Saved state for {len(adapted_state)} modules\")\n",
    "\n",
    "# Save to file\n",
    "torch.save(adapted_state, 'fluid_weights_adapted.pt')\n",
    "print(\"Saved to 'fluid_weights_adapted.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To reload later:\n",
    "# loaded_state = torch.load('fluid_weights_adapted.pt')\n",
    "# fluid_model.load_fluid_state(loaded_state)\n",
    "# print(\"State restored!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ablation Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different configurations\n",
    "ablation_configs = {\n",
    "    'full': FluidConfig(\n",
    "        learning_rate=1e-5,\n",
    "        use_attention_guided_plasticity=True,\n",
    "        use_temporal_surprise=True,\n",
    "        use_contextual_homeostasis=True,\n",
    "    ),\n",
    "    'no_agp': FluidConfig(\n",
    "        learning_rate=1e-5,\n",
    "        use_attention_guided_plasticity=False,\n",
    "        use_temporal_surprise=True,\n",
    "        use_contextual_homeostasis=True,\n",
    "    ),\n",
    "    'no_tsm': FluidConfig(\n",
    "        learning_rate=1e-5,\n",
    "        use_attention_guided_plasticity=True,\n",
    "        use_temporal_surprise=False,\n",
    "        use_contextual_homeostasis=True,\n",
    "    ),\n",
    "    'no_ch': FluidConfig(\n",
    "        learning_rate=1e-5,\n",
    "        use_attention_guided_plasticity=True,\n",
    "        use_temporal_surprise=True,\n",
    "        use_contextual_homeostasis=False,\n",
    "    ),\n",
    "    'base_only': FluidConfig(\n",
    "        learning_rate=1e-5,\n",
    "        use_attention_guided_plasticity=False,\n",
    "        use_temporal_surprise=False,\n",
    "        use_contextual_homeostasis=False,\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(\"Ablation configurations defined:\")\n",
    "for name in ablation_configs:\n",
    "    print(f\"  - {name}\")\n",
    "\n",
    "print(\"\\nTo run ablation studies, reload model and use different configs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Stability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze stability metrics\n",
    "print(\"=\" * 60)\n",
    "print(\"STABILITY ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, lora in list(fluid_model.fluid_loras.items())[:5]:\n",
    "    stability = lora.stability.check_stability(lora.A.data, lora.B.data)\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Weight norm: {stability.weight_norm:.4f}\")\n",
    "    print(f\"  Spectral norm: {stability.spectral_norm:.4f}\")\n",
    "    print(f\"  Drift from origin: {stability.drift_from_origin:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check homeostasis status\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"HOMEOSTASIS STATUS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "burned_in = sum(1 for lora in fluid_model.fluid_loras.values() if lora.state.is_burned_in)\n",
    "print(f\"\\nModules with completed burn-in: {burned_in}/{len(fluid_model.fluid_loras)}\")\n",
    "\n",
    "for name, lora in list(fluid_model.fluid_loras.items())[:3]:\n",
    "    if lora.state.target_mean is not None:\n",
    "        mean_drift = torch.norm(lora.state.activation_mean - lora.state.target_mean).item()\n",
    "        var_drift = torch.norm(lora.state.activation_var - lora.state.target_var).item()\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Mean drift: {mean_drift:.6f}\")\n",
    "        print(f\"  Variance drift: {var_drift:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates the **Fluid Weights** system for perpetual plasticity in transformers.\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Weights can adapt during inference** without explicit loss functions\n",
    "2. **Stability mechanisms prevent divergence** (EWC, spectral norm, adaptive rate)\n",
    "3. **Novel mechanisms enhance learning**:\n",
    "   - Attention-Guided Plasticity uses attention patterns as learning signals\n",
    "   - Temporal Surprise creates implicit prediction objectives\n",
    "   - Contextual Homeostasis maintains stable activation statistics\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "- Learning is subtle and requires multiple exposures\n",
    "- Hyperparameters require tuning for different tasks\n",
    "- Long-term stability over millions of tokens needs more testing\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Test on longer conversations\n",
    "2. Tune hyperparameters for faster adaptation\n",
    "3. Add more sophisticated evaluation metrics\n",
    "4. Test on real-world adaptation scenarios (user style, domain adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFluid Weights Demo Complete!\")\n",
    "print(\"\\nThis is exploratory research. Results may vary.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
